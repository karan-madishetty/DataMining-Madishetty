
==============================

- ***Name:*** Karan Madishetty
- ***Roster Number:*** M20228991

|   #   |Group Names |
|:----:|:------------------|
|    1 | Muni Bhupathi Reddy Dandu     (M20228454)                  |
|    3 | Karan Madishetty              (M20228991)           |

----


### What is Spatial Transformer Network?

Spatial Transformer Network (STN) is a differentiable module that can be inserted anywhere in ConvNet(CNN's) architecture to increase its geometric invariance. It effectively gives the network the ability to spatially transform _(Operations Such as Scaling, Rotation)_ feature maps at no extra data or supervision cost.

Reference:https://github.com/kevinzakka/spatial-transformer-network
___

### What are the three key features of STN that solves the issues of Convolutional Networks?
The Spatial Transformer mechanism addresses the issues by providing Convolutional Neural Networks with explicit spatial transformation capabilities. It possesses 3 defining properties that make it very appealing.

### Modular: 
STNs can be inserted anywhere into existing architectures with relatively small tweaking.

### Differentiable: 

STNs can be trained with backprop allowing for end-to-end training of the models they are injected in.

### Dynamic:

 STNs perform active spatial transformation on a feature map for each input sample as compared to the pooling layer which acted identically for all input samples.
___
### What are the three components of STN?

## Localization network: 
_(General Def)It will estimate which are the best parameters given the input data should be applied to this transformation_


Takes the feature map as input and outputs the parameters of the affine transformation that should be applied to that feature map.

## Grid generator: 
_(General Def)It Basically defines what kind of transformation to be applied_

Generates a grid of (x,y) coordinates using the parameters of the affine transformation that corresponds to a set of points where the input feature map should be sampled to produce the transformed output feature map.

## Sampler: 
_(General Def) It Does Some operations that will solve this problem with the values that are not defined.After Transformations are done the resultant feature map is gone through this phase to take the errors out of the input feature map_

Takes as input the input feature map and the grid generated by the grid generator and produces the output feature map using bilinear interpolation.
___
### Goal of Spatial Transformer networks

The goal of spatial transformers is to add to your base network a layer able to perform an explicit geometric transformation on an input. The parameters of the transformation are learnt thanks to the standard backpropagation algorithm, meaning there is no need for extra data or supervision.

<a href="https://ibb.co/bA60oG"><img src="https://preview.ibb.co/gRSUhb/Capture.png" alt="Capture" border="0"></a><br />
___
### Affine Transformation
An affine transformation is any transformation that preserves collinearity (i.e., all points lying on a line initially still lie on a line after transformation) and ratios of distances (e.g., the midpoint of a line segment remains the midpoint after transformation)
Reference:http://mathworld.wolfram.com/AffineTransformation.html
___
### MNIST
The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning.
Reference:https://en.wikipedia.org/wiki/MNIST_database
___
### BackPropagation
Backpropagation is a method used in artificial neural networks to calculate the error contribution of each neuron after a batch of data (in image recognition, multiple images) is processed. It is a special case of an older and more general technique called automatic differentiation.
Reference:https://en.wikipedia.org/wiki/Backpropagation
___
Also Contributed Content On Poolin 
Max-Pooling
Avg-pooling

Another power tool that CNNs use is called pooling. Pooling is a way to take large images and shrink them down while preserving the most important information in them. The math behind pooling is second-grade level at most. It consists of stepping a small window across an image and taking the maximum value from the window at each step. In practice, a window 2 or 3 pixels on a side and steps of 2 pixels work well.

After pooling, an image has about a quarter as many pixels as it started with. Because it keeps the maximum value from each window, it preserves the best fits of each feature within the window. This means that it doesnâ€™t care so much exactly where the feature fit as long as it fit somewhere within the window. The result of this is that CNNs can find whether a feature is in an image without worrying about where it is. This helps solve the problem of computers being hyper-literal.

![](http://brohrer.github.io/images/cnn9.png)

A pooling layer is just the operation of performing pooling on an image or a collection of images. The output will have the same number of images, but they will each have fewer pixels. This is also helpful in managing the computational load. Taking an 8 megapixel image down to a 2 megapixel image makes life a lot easier for everything downstream.
